# AIC-assignment


SOP :

Hi , I’m Abakash From Mech. Department . I’m interested to join AI community because I want to Learn About Tech. Machine Learning & AI & Use it to solve Various Problem faced By People. I want to Explore this Domain & How It can Help Us . I find It easy to Learn For Me , The assignment really ignited the interest to Join AI community . I have Explored MoodI , TechFest & various Things During My Freshie Year & realised , I want to give my Time & Energy , Learning About Technology , & it’s Various implementation .But Why Not Tech Teams then ? I find It easier To learn ML , maybe bcoz I’m Interested in it . Earlier I didn’t wanted To give time to Tech teams , but AI community Is what I Like To give my Time to . AI Community IIT B is a Perfect Place For That . Interested Of My Interest , I started Learning ML , as soon As summer Started & Didn’t Knew Much About Earlier , But created a Model Using (Pandas) Earlier . The Assignment Of AI community was Interesting , It generated an Excitement to Learn Things Further & Explore More & I did Learned To Use TensorFlow Much during This Time . I reached Out to seniors to Find Resources To Learn , Even Though It seemed Easy To Learn , I instead Choose to Learn by seeing Every single Tutorials & view it in Detail . And Joining This , I also Wat to Connect To Like Minded Individuals , Who like to learn about AI , ML , etc & Are inclined to Problem Solving , Using Different Approaches .



Hackathon (Non-Tech Q1 ) :

For a idea , Let’s say the Problem statement is “To Predict a Fraudlent Transaction “  . Before entering a Hackathon , I would research aa bit about the Organisers , companies That are conducting the Hackathon & research about their aim , What do they want , Their previous  Hackathon’s Winners . Then , then the Task is Searching For a Dataset , with relevance , For this problem statement , a dataset On Fraudlent Transaction already exists on Kaggle.com .  I'll search for Dataset , which are diverse , large & give much info about fraudlent transactions. Search on various websites Kaggle , Twitter API, etc .This Would Give Us a dataset For Training our Model.Then I'll do Preprocessing of data , Remove some missing value Columns,noise . Then I will Do the EDA(Exploratory Data Analysis) of the Training Dataset to gainn insights into data , determine the frequency of Fraudlent transaction, Determine the Variables & various Dependencies that can Be Used To Accurately make Predictions , Identify different Ports From Which Fraudlent transaction are made & if they are somehow related , can they be predicted ? For This I will Use matplot lib , numpy , heatmap ,scatter plot , Histograms To Find out The Correct Dependencies , For example , If a transaction is made from terminals in this Range Of No. Then it’s a fake transaction(These terminals / ports doesn’t exist) . Using Codes , I will clean The data points that are empty in a given Column , by checking datatypes of entries , 	We can determine , If a column is empty(“?” filled inside) or Nan , Then We’ll remove such Data , These alongwith Dataset searching , Should be done in 6-7 days . Then I’ll research on various Models Convolution Neural Networks(CNN) , Transformer based models , Which one Will be best for Us To Implement , based On Our Need , Learn how to Implement them to our Benefit . Then I’ll train the Model On the Dataset , Test It using a part of the Same Dataset , Then A different Dataset , then Again Train On Another Dataset , determine accuracy , Train it based On the actual outputs Of the Test Data sets. I'll fine-tune hyper parameters , Using Techniques like grid search , random search or bayesian Optimisation to improve the model . Fix issues If Found & Determine , How to Increase Accuracy . These should Be where I put max Time  to Increase the Accuracy Of Our Model & improve the model to make most correct predictions . I'll  Implement techniques to handle class imbalance if present in the dataset, such as oversampling, undersampling, or using class weights . I'll regularize the models to prevent overfitting by applying techniques like dropout, batch normalization, or early stopping.  Validate the models using cross-validation and performance metrics like accuracy, precision, recall, and F1-score . These Should be completed Within 19-20 days Of Start of Hackathon(If It’s 1 Month) . Then I’ll Test The model In real life situations . I'll Evaluate the models on the test dataset and compare their performance metrics , Select the best-performing model based on overall accuracy and generalization. Then Document the entire process, including data preprocessing, model selection, hyperparameters, and evaluation metrics, for future reference. Prepare a presentation highlighting the problem statement, approach, methodology, results, and future improvements. Practice the presentation and anticipate questions from the judges or audience. I'll Fine-tune the presentation based on feedback. Then I'll Set up the deployment environment for the chosen model, & ensure compatibility with the hosting platform and scalability. Finalize the deployment by integrating the model with the user interface and conducting end-to-end testing to ensure functionality and performance.  Collabrating With Various Platforms Which face Fraudlent Transactions(Earlier Amazon, ebay used to Face) Check the transactions Using such Platfroms . Take feedback & Ask suggestion For Improvements , & then Implements The Improvements .





Tech :

Q3)  For a 2D grid , With Well Defined Start & End Point , I would Choose Manhattan Distance Metrics , Bcoz It's a grid & We can Move only along X & Y axes .
      This Metrics easily tell me the comparision b/w lengths of 2 paths , in a grid .
      Some Other Metrics Are :
      1)Euclidean Distance
      2)Chebyshev Distance
      3)Minkowski Distance
      4)Cosine similarity
      5)Jaccard Distance


    The criteria for Similarity b/w 2 vectors used , is cosine similarity , its the angle b/w 2 vectors divide by their magnitude . If cosine similarity Of 2 vectors is Higher , Then Their Likeliness is more 







sources :
    freecodecamp.org(I tried to learn CNN , but unable to implement in time , due to unable to install dataset , library , etc)
    medium.com
    random google search

